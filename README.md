# Multivariable Calculus Final Project: Exposition on Gradient Descent

## Overview
This repository contains the final project for our Multivariable Calculus class, focused on explaining the concept of gradient descent. Gradient descent is a fundamental optimization algorithm widely used in various fields including machine learning, optimization, and physics. In this project, we aim to provide a comprehensive exposition on gradient descent, covering its mathematical underpinnings, practical applications, and implementation.

## Project Structure

## Presentation
The presentation slides (`presentation.pdf`) offer a concise overview of gradient descent, including its definition, mathematical formulation, convergence properties, and practical considerations. We also highlight key applications and variants of gradient descent, such as stochastic gradient descent and mini-batch gradient descent.

## Code Implementation
The `code/` directory contains Python implementations of gradient descent algorithm and an example notebook illustrating its usage. `gradient_descent.py` provides a modular implementation of gradient descent that can be easily integrated into various optimization tasks. The accompanying Jupyter notebook `example_usage.ipynb` demonstrates how to use gradient descent to minimize a simple convex function.

## Report
The `report/` directory contains a detailed written report (`report.pdf`) discussing gradient descent in depth. The report covers mathematical derivations, intuition behind the algorithm, comparison with other optimization methods, and real-world applications. Additionally, it includes experimental results and discussions on hyperparameter tuning and convergence analysis.

## Contributors
- [John Doe](https://github.com/johndoe)
- [Jane Smith](https://github.com/janesmith)

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
