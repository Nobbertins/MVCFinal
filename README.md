# Gradient Descent
and two examples of its use (one clear/easy, one non-trivial), and explaining how/why it works and when it can run into problems.

## Python Implementations
`basicGradientDescent.py`: easy application implementation\
`randomSamplingGradientDescent.py`: nontrivial application implementation\
`gradientDescentSimulation.py`: demo implementation

## Mathematica Visualizations
`easy application.nb`: easy application visualization \
`nontrivial application.nb: nontrivial application visualization

## Presentation
`slides.pdf`: pdf of the slides used\
`/slides`
>`s1_example`: example surface (aesthetically pleasing)\
>`s5_paraboloid`: render of a simple paraboloid\
>`s5_paraboloid_levelset`: ContourPlot of the paraboloid's level set\
>`s6_learning_rate1`: side view of gradient descent, low learning rate\
>`s6_learning_rate2`: side view of gradient descent, medium learning rate\
>`s6_learning_rate3`: side view of gradient descent, high learning rate\
>`s7_landscape`: render of a more complicated surface\
>`s7_landscape_levelset`: ContourPlot of the landscape's level set\
>`s8_samples`: visualization of 100 starting points on the surface\
>`s8_samples_detail`: showing an optimal gradient descent from a random starting point\
>`s8_two_mins`: showing the difference between Mathematica's minimum (red) and our minimum (cyan)

  

## Contributors
- [Ayaan Dhuka](https://github.com/Nobbertins)
- [Enbao Cao](https://github.com/ecao77)
